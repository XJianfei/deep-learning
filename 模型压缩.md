# 模型压缩
深度学习模型：1.计算量非常巨大；2.模型本身较大特别吃内存；这两个原因，使得很难把NN应用到嵌入式系统中去，因为嵌入式系统资源有限，而NN模型动不动就好几百兆。

目前，我们实际上已经在商业应用程序中部署了许多模型。训练的计算需求随着研究人员的数量而增长，但推理所需的周期与用户成比例地扩大。这意味着纯粹的推理效率已经成为许多团队的一个亟待解决的问题。  

## 模型压缩
Prunes the network：只保留一些重要的连接；  
Quantize the weights：通过权值量化来共享一些weights；  
Huffman coding：通过霍夫曼编码进一步压缩；  

### 参数修剪（Pruning）
通过剪枝，可以实现对模型压缩而不降低模型准确率  
![](https://i.imgur.com/UE4R36s.jpg)  
从上图的左边的pruning阶段可以看出，其过程是：  
1. 正常的训练一个网络；  
2. 把一些权值很小的连接进行剪枝：通过一个阈值来剪枝；  
3. retrain 这个剪完枝的稀疏连接的网络；  
4. 重复以上过程，知道网络参数变成一个高度稀疏的矩阵；  

为了进一步压缩，对于weight的index，不再存储绝对位置的index，而是存储跟上一个有效weight的相对位置，这样index的字节数就可以被压缩了。  
对于卷积层用 8bits 来保存这个相对位置的index，在全连接层中用 5bits 来保存；  
![](https://i.imgur.com/ZoQHfj4.jpg)  
上图是以用3bits保存相对位置为例子，当相对位置超过8（3bits）的时候，需要在相对位置为8的地方填充一个0，防止溢出；8bit和5bit同样适用这样的方式防止溢出。  

### 量化和权重共享（Trained Quantization and Weight Sharing）
#### 量化/定点化

##### 量化为什么有效
深度网络的一个神奇特质是它们倾向于很好地应对输入中的高噪声。如果您考虑识别您刚刚拍摄的照片中的物体，网络必须忽略所有CCD噪音，灯光变化以及它与之前看到的训练样例之间的其他非本质差异，并专注于重要事项相似之处。这种能力意味着它们似乎将低精度计算视为另一种噪声源，并且即使使用容量较少的数字格式仍能产生准确的结果。

##### 为什么量化？
**神经网络模型可占用磁盘上的大量空间**，例如原始AlexNet以浮点格式超过200 MB。几乎所有这些尺寸都被神经连接的权重所占用，因为在单个模型中通常有数百万个。因为它们的浮点数略有不同，所以像zip这样的简单压缩格式不能很好地压缩它们。它们排列成大层，并且在每层内，重量倾向于正常分布在一定范围内，例如-3.0至6.0。

量化的最简单动机是通过存储每层的最小值和最大值来缩小文件大小，然后将每个浮点值压缩为8位整数，表示该范围内256的线性集合中最接近的实数。例如，在-3.0到6.0范围内，0字节表示-3.0，255表示6.0，128表示约1.5。我将在稍后进行精确的计算，因为它有一些细微之处，但这意味着你可以获得磁盘上收缩75％的文件的好处，然后在加载后转换回float，以便你现有的浮点代码无需任何改动即可工作。

量化的另一个原因是通过完全使用8位输入和输出运行它们来减少进行推理计算所需的计算资源。这要困难得多，因为它需要在您进行计算的任何地方进行更改，但会提供很多潜在的奖励。获取8位值只需要浮点数的25％的内存带宽，因此您可以更好地使用缓存并避免RAM访问的瓶颈。您通常还可以使用SIMD操作，每个时钟周期执行更多操作。在某些情况下，您将拥有可用于加速8位计算的DSP芯片，这可以提供许多优势。

将计算移动到8位将有助于您更快地运行模型，并使用更少的功率（这在移动设备上尤为重要）。它还为许多无法有效运行浮点代码的嵌入式系统打开了大门，因此它可以在物联网世界中实现许多应用程序。

##### 为什么不直接以较低的精度训练？
在低位深度进行了一些实验训练，但结果似乎表明您需要高于8位来处理反向传播和梯度。这使得实施培训更加复杂，因此从推理开始是有道理的。我们已经有很多浮动模型已经使用和熟悉，因此能够直接转换它们非常方便。  

##### 权重量化实现
**一般量化实现**  
通过对权重进行量化来进一步压缩网络（量化可以降低表示数据所用的位数）  
![](https://i.imgur.com/WnAfSU1.jpg)  
假设我们输入神经元有4个，输出神经元也是4个，那么我们的weight应该是4x4，同样梯度也是。  
把 weight矩阵 聚类成了4个cluster（由4种颜色表示）。属于同一类的weight共享同一个权值大小（看中间的白色矩形部分，每种颜色权值对应一个cluster index）；由于同一cluster的weight共享一个权值大小，所以我们只需要存储权值的index，例子中是4个cluster，所以原来每个weight需要32bits，现在只需要2bits，非常简单的压缩了16倍  
而在 权值更新 的时候，所有的gradients按照weight矩阵的颜色来分组，同一组的gradient做一个相加的操作，得到是sum乘上learning rate再减去共享的centroids，得到一个fine-tuned centroids，这个过程看上图，画的非常清晰了。  
实际中，对于AlexNet，卷积层quantization到8bits（256个共享权值），而全连接层quantization到5bits（32个共享权值），并且这样压缩之后的网络没有降低准确率  
**TensorFlow 8bit量化实现**  
先计算每个层的左小指和最大值，将最大值和最小值范围空间内线性划分为256段（2^8），每段用唯一的8bit整数表示该段内的实数值，加载模型后转换回浮点数  

##### 运算量化实现
通过编写推理期间常用的等效8位版本的操作来实现量化。这些包括卷积，矩阵乘法，激活函数，池操作和连接。转换脚本首先用量化的等价物替换它所知道的所有单个操作。这些是小型子图，在浮点数和8位之间移动数据之前和之后都有转换函数。下面是它们的样子。首先是这里的原始Relu操作，具有浮动输入和输出：  
![](https://i.imgur.com/MGGBhfO.png)  
然后，这是等效的转换子图，仍然具有浮点输入和输出，但具有内部转换，因此计算以8位完成。  
![](https://i.imgur.com/3fBI2qN.png)  
min和max操作实际上查看输入浮点张量中的值，然后将它们提供给Dequantize操作，该操作将张量转换为8位。  

转换单个操作后，下一步是删除与float之间的不必要转换。如果有连续的操作序列都具有浮点等价物，则会有很多相邻的Dequantize / Quantize操作。这个阶段发现了这个模式，认识到他们互相取消，然后删除它们，如下所示：  
![](https://i.imgur.com/Lt2AW28.png)  
大规模应用于所有操作都具有量化等价物的模型，这给出了一个图形，其中所有张量计算都以8位完成，而不必转换为浮点数  

##### 如何处理8bit运算可能的越界问题
关于最小和最大范围的好处是它们通常可以预先计算。权重参数是加载时已知的常数，因此它们的范围也可以存储为常量。我们经常知道输入的范围（例如图像通常是在0.0到255.0范围内的RGB值），并且许多激活函数也具有已知范围。这可以避免必须分析操作的输出以确定范围，我们需要对数学运算进行处理，例如卷积或矩阵乘法，它们从8位输入产生32位累加结果。  

如果你在8位输入上进行任何类型的算术运算，你自然会开始累积超过8位精度的结果。如果添加两个8位值，则结果需要9位。如果将两个8位数相乘，则输出中会得到16位。如果总计一系列8位乘法，就像我们对矩阵乘法一样，结果会超过16位，累加器通常需要至少20到25位，具体取决于所涉及的点产品的长度。  

这可能是我们量化方法的一个问题，因为我们需要采用比8位宽得多的输出并将其缩小以进入下一个操作。对矩阵乘法进行此操作的一种方法是计算最大和最小可能的输出值，假设所有输入值都处于极值。这是安全的，因为我们在数学上知道没有结果可以超出这个范围，但实际上大多数权重和激活值分布更均匀。这意味着我们看到的实际值范围远小于理论值，所以如果我们使用更大的边界，我们就会在从未出现的数字上浪费很多8位。相反，我们使用QuantizeDownAndShrinkRange运算符来获取32位累积张量，对其进行分析以了解所使用的实际范围，并重新缩放，以便8位输出张量有效地使用该范围。有些策略涉及观察大量训练数据遇到的实际最小值和最大值，并对其进行硬编码以避免每次都在分析缓冲区的范围。  

##### 如何舍入
在量化的过程中，我们遇到的最难且最微妙的一个问题是偏差的累积。前面提过，神经网络对噪声不敏感，但如果对舍入操作不小心，会导致偏差往某个方向累积，最终影响精度。让舍入后的输入值减去舍入后的最小值，而不是让输入值减去最小值然后才做舍入。

#### 权重共享（Weight Sharing）
具体是怎么做的权值共享，或者说是用什么方法对权值聚类的呢？  
其实就用了非常简单的 K-means，对每一层都做一个weight的聚类，属于同一个 cluster 的就共享同一个权值大小。  
**跨层的weight不进行共享权值**  

> Initialization of Shared Weights  
> 做过 K-means 聚类的都知道，初始点的选择对于结果有着非常大的影响，在这里，初始点的选择同样会影响到网络的性能。   
> 作者尝试了很多生产初始点的方法：Forgy(random), density-based, and linear initialization.  
> ![](https://i.imgur.com/nWpa7lx.jpg)  
> 画出了AlexNet中conv3层的权重分布，横坐标是权值大小，纵坐标表示分布，其中红色曲线表示PDF（概率密度分布），蓝色曲线表示CDF（概率密度函数），圆圈表示的是centroids：黄色（Forgy）、蓝色（density-based）、红色（linear）。  
> 作者提到：大的权值往往比小的权值起到更重要的作用，不过，大的权值往往数量比较少；  
> 可以从图中看到，Forgy 和 density-based 方法产生的centroids很少落入到大权值的范围中，造成的结果就是忽略了大权值的作用；而Linear initialization产生的centroids非常平均，没有这个问题存在；  
> 后续的实验结果也表明，Linear initialization 的效果最佳。  
> 
> **8bit Linear initalization**  
> 先计算每个层的左小指和最大值，将最大值和最小值范围空间内线性划分为256段（2^8），每段用唯一的8bit整数表示该段内的实数值，加载模型后转换回浮点数

### 霍夫曼编码（Huffman Coding）
Huffman Coding 是一种非常常用的无损编码技术。霍夫曼编码使用变长编码表对源符号（如文件中的一个字母）进行编码,其中变长编码表是通过一种评估来源符号出现机率的方法得到的，出现机率高的字母使用较短的编码，反之出现机率低的则使用较长的编码，这便使编码之后的字符串的平均长度、期望值降低，从而达到无损压缩数据的目的。    
[Hoffman编码](http://www.myexception.org/program/826864.html)   

### SVD分解(奇异值分解)
奇异值分解用于压缩全连接层  
SVD中，（u × v）权重矩阵W参数化的层近似地分解为：  
![](https://i.imgur.com/hGYYIYX.png)  
U为（u × t）矩阵，Σt为（t × t）矩阵，V为（v x t）矩阵，SVD将计算量从u*v减小到t*(u+v),t小于min(u,v)  
在实现时，相当于把一个全连接层拆分成两个，中间以一个低维数据相连。  
![](https://i.imgur.com/2DBDZ9k.png)  


模型压缩方法总结起来有以下几个方式：

改变网络结构，矩阵分解，权重稀疏化（接近0的权重置0），权重量化（二值，三值，8-bit，聚类量化），1*1小卷积：参数量化之后一般需要对模型进行retrain

参考：https://zhuanlan.zhihu.com/p/27423806  

 

