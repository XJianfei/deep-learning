# Dropout
dropout是一种防止模型过拟合的技术，这项技术也很简单，但是很实用。  

dropout是指在模型训练过程中，随机让隐藏层节点权重变为0，暂时认为这些节点不是网络结构一部分，但会把全红保留下来（不更新）；作用在激活函数之后。  

预测时需要注意。因为我们训练的时候会随机的丢弃一些神经元，但是预测的时候就没办法随机丢弃了（如果在预测时丢弃，会使结果不稳定，也就是给定一个测试数据，有时候输出a有时候输出b，结果不稳定，这是实际系统不能接受的，用户可能认为你的模型有”bug“）。一种”补偿“的方案就是每个神经元的输出都乘以一个p，这样在”总体上“使得测试数据和训练数据是大致一样的。比如一个神经元的输出是x，那么在训练的时候它有p的概率keep，(1-0)的概率丢弃，那么它输出的期望是p x+(1-p) 0=px。因此测试的时候把这个神经元乘以p可以得到同样的期望。但是这样测试的时候就需要多一次乘法，我们对于训练的实时性要求没有测试那么高。所以更为常见的做法是**训练的时候给U1的每个元素除以p**，相当于给H1放大1/p倍，那么预测的时候，那么后面层的参数学到的就相当于没有dropout的情况，因此预测的时候就不需要再乘以p了  

参考：https://blog.csdn.net/qq_14821323/article/details/80015224
