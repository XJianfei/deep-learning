# batch normal

网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，因此我们不得不降低学习率、小心地初始化。  

数据归一化方法让数据具有0均值和单位方差，如果简单的这么干，会降低层的表达能力。比如，在使用sigmoid激活函数的时候，如果把数据限制到0均值单位方差，那么相当于只使用了激活函数中近似线性的部分，这显然会降低模型表达能力。  
![](https://i.imgur.com/T8XgLbs.png)  
![](https://i.imgur.com/kcN5jtT.png)  
为此，作者又为BN增加了2个参数，用来保持模型的表达能力。   
于是最后的输出为：   
![](https://i.imgur.com/QPnhQYv.png)  
上述公式中用到了均值E和方差Var，需要注意的是理想情况下E和Var应该是针对整个数据集的，但显然这是不现实的。因此，作者做了简化，用一个Batch的均值和方差作为对整个数据集均值和方差的估计。   
整个BN的算法如下：（e为一个很小的整数，保证不除零）   
![](https://i.imgur.com/Tgtv7WH.png)  

## 测试过程
实际测试网络的时候，我们依然会应用下面的式子：  
![](https://i.imgur.com/nL9N4vT.png)  
这里的均值和方差已经不是针对某一个Batch了，而是针对整个数据集而言。因此，在训练过程中除了正常的前向传播和反向求导之外，我们还要记录每一个Batch的均值和方差，以便训练完成之后按照下式计算整体的均值和方差：  
![](https://i.imgur.com/mWqJlMI.png)   
作者在文章中说应该把BN放在激活函数之前，这是因为Wx+b具有更加一致和非稀疏的分布。但是也有人做实验表明放在激活函数后面效果更好。这是实验链接，里面有很多有意思的对比实验：https://github.com/ducha-aiki/caffenet-benchmark  

BN统一了各层的方差，以适用一个统一的学习率，作用在激活函数之前，防止否个特征对网络优化起到主导作用，可以选择较大的初始学习率，加快训练速度，不需要适用dropout和L2正则化。  




https://arxiv.org/pdf/1502.03167.pdf

参考：https://blog.csdn.net/u014114990/article/details/52290064