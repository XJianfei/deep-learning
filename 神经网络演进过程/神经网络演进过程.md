# 神经网络演变过程 
![](https://i.imgur.com/K3BMSiN.png)  

## LeNet
输入层：32×32。  
卷积层：3个。  
降采样层：2个。  
全连接层：1个。  
输出层（高斯连接）：10个类别（数字0～9的概率）。  
![](https://i.imgur.com/ax0pFnR.png)  
（1）输入层。输入图像尺寸为32×32。这要比 MNIST 数据集中的字母（28×28）还大，即对图像做了预处理 reshape 操作。这样做的目的是希望潜在的明显特征，如笔画断续、角点，能够出现在最高层特征监测卷积核的中心。  
（2）卷积层（C1, C3, C5）。卷积运算的主要目的是使原信号特征增强，并且降低噪音。在一个可视化的在线演示示例[5]中，我们可以看出不同的卷积核输出特征映射的不同，如图 6-5所示。  
（3）下采样层（S2, S4）。下采样层主要是想降低网络训练参数及模型的过拟合程度。通常有以下两种方式。
**最大池化（max pooling）**：在选中区域中找最大的值作为采样后的值。
**平均值池化（mean pooling）**：把选中的区域中的平均值作为采样后的值。
（4）全连接层（F6）。F6 是全连接层，计算输入向量和权重向量的点积，再加上一个偏置。随后将其传递给 sigmoid 函数，产生单元 i 的一个状态。  
（5）输出层。输出层由欧式径向基函数（Euclidean radial basis function）单元组成，每个类别（数字的0～9）对应一个径向基函数单元，每个单元有84个输入。也就是说，每个输出 RBF 单元计算输入向量和该类别标记向量之间的欧式距离。距离越远，RBF输出越大。  

## AlexNet
AlexNet 的结构如图所示。图中明确显示了两个 GPU 之间的职责划分：一个 GPU 运行图中顶部的层次部分，另一个 GPU 运行图中底部的层次部分。GPU 之间仅在某些层互相通信。  
![](https://i.imgur.com/GyNbomY.png)  
AlexNet 由5个卷积层、5个池化层、3个全连接层，大约5000万个可调参数组成。最后一个全连接层的输出被送到一个1000维的 softmax 层，产生一个覆盖1000类标记的分布  

## VGGNet  
VGGNet 可以看成是加深版本的 AlexNet，VGGNet 使用的层更多，通常有16～19层，而 AlexNet 只有8层。VGGNet 的结构如图所示。
![](https://i.imgur.com/NZPu50a.png)  

## [NIN（Network In Network）](https://www.jianshu.com/p/8a3f9f06bbe3)
NIN的总体结构是一系列mplconv层的堆叠。被称作“Network In Network”（NIN），因为内部含有MLP。  
对传统的卷积方法做了两点改进：  
1. 将原来的线性卷积层（linear convolution layer）变为多层感知卷积层（multilayer perceptron）  
2. 将全连接层的改进为全局平均池化。没有采用传统CNN的全连接层进行分类，而是直接通过全局平均池化层（GAP）输出最后一个mlpconv层特征图的空间平均值作为类别的置信度值，然后将得到的向量输入softmax层。  
    
这使得卷积神经网络向另一个演化分支——增强卷积模块的功能的方向演化  
![](https://i.imgur.com/jzYkzJt.png)  

CNN的卷积滤波器是底层数据块的广义线性模型（generalized linear model ）（GLM），而且我们认为它的抽象程度较低。这里的抽象较低是指该特征对同一概念的变体是不变的。用更有效的非线性函数逼近器代替GLM可以增强局部模型的抽象能力。当样本的隐含概念（latent concept）线性可分时，GLM可以达到很好的抽象程度，例如：这些概念的变体都在GLM分割平面的同一边，而传统的CNN就默认了这个假设——认为隐含概念（latent concept）是线性可分的。然而，同一概念的数据通常是非线性流形的（nonlinear manifold），捕捉这些概念的表达通常都是输入的高维非线性函数。NIN中将原来的线性卷积层（linear convolution layer）变为多层感知卷积层（multilayer perceptron）  
**个人认为实现NIN就是在原来的卷积后再加上1*1的卷积（Mlpconv可以理解为1*1卷积）即可**
NIN网络结构：  
![](https://i.imgur.com/pVn4B05.png)    


## GoogLeNet
https://blog.csdn.net/u010349092/article/details/81603993  
https://blog.csdn.net/u010349092/article/details/81604732  
https://blog.csdn.net/u010349092/article/details/81604878  
https://blog.csdn.net/u010349092/article/details/81605108  

## ResNet
按照一般的经验，只要没有发生梯度消失或者梯度爆炸，并且不过拟合，网络应该是越深越好。但是，在 CIFAR10 上训练网络时却发现，层数从20层增加到56层，错误率上升了，准确率下降了，这种情况被称为**网络退化**（network degradation），为此 ResNet 中引入了一个 shortcut 结构，将输入跳层传递与卷积的结果相加。  
ResNet 中残差（residual）的含义，其实是如果能用几层网络去逼近一个复杂的非线性映射 H(x) 来预测图片的分类，那么同样可以用这几层网络去逼近它的残差函数（residual function）F(x)= H(x)−x，并且我们认为优化残差映射要比直接优化 H(x) 简单。  

## 增加新的功能模块
增加新的功能模块这个演化方向主要涉及 FCN（反卷积）、STNet、CNN 与 RNN/LSTM 的混合架构。   

