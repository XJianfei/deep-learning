# 循环神经网络
循环神经网络主要是自然语言处理（natural language processing，NLP）应用的一种网络模型。它不同于传统的前馈神经网络（feed-forward neural network，FNN），循环神经网络在网络中引入了定性循环，使信号从一个神经元传递到另一个神经元并不会马上消失，而是继续存活。   

循环神经网络隐藏层的输入不仅包括上一层的输出，还包括上一时刻该隐藏层的输出。理论上，循环神经网络能够包含前面的任意多个时刻的状态，但实践中，为了降低训练的复杂性，一般只处理前面几个状态的输出。  
![](https://i.imgur.com/GEfC8XJ.png)   
循环神经网络的训练也是使用误差反向传播（backpropagation，BP）算法，并且参数 w1、w2 和 w3 是共享的。但是，其在反向传播中，不仅依赖当前层的网络，还依赖前面若干层的网络，这种算法称为随时间反向传播（backpropagation through time，BPTT）算法。BPTT 算法是 BP 算法的扩展，可以将加载在网络上的时序信号按层展开，这样就使得前馈神经网络的静态网络转化为动态网络。  

# 循环神经网络发展
循环神经网络的发展如图所示。  
![](https://i.imgur.com/FoyLBCV.png)  

## 增强隐藏层的功能

**循环神经网络折叠结构：**  
![](https://i.imgur.com/Q7Z4o1I.png)  
\bar{x}表示输入向量，\bar{y} 表示输出向量，而\bar{s}表示状态向量。  
W_x​是连接输入层到状态层的权重矩阵。  
W_y是连接状态层到输出层的权重矩阵。  
W_s表示连接之前时间步长状态到当前时间步长状态的权重矩阵。  

**循环神经网络基于时间展开模型：**  
![](https://i.imgur.com/tJJDwXR.png)  

**循环神经网络计算公式：**  
\bar{s}_t = \Phi (\bar{x}_tW_x + \bar{s}_{t-1}W_s)  
\bar{y}_t = \sigma (\bar{s}_tWy)   
![](https://i.imgur.com/DPIdqO1.png)  

### 简单 RNN(ELman network)
三层基本神经网络的反馈作为存储输入，叫做Elman网络，如下图所示：    
![](https://i.imgur.com/nRgyTKn.png)  


### LSTM
![](https://i.imgur.com/K68dSZo.png)  
![](https://i.imgur.com/NuaO39Q.png)  


![](https://i.imgur.com/jRQSuNF.png)   
使用lstm步骤：  
1. 获取数据，数据格式为（seq_len，batch_size,embed_size）,输入序列长度，分别为批大小，输入向量大小。  
2. 定义LSTM网络，lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=n_layers)   
input_dim：输入序列长度  
hidden_size：隐藏状态的大小; 每个LSTM单元在每个时间步产生的输出数。  
n_layers：lstm层数，通常是1到3之间的数，默认为1   
3. lstm调用output,(h,c) = lstm(input.view(1, 1, -1), (h0, c0))   
input：输入tensor，（seq_len，batch_size,embed_size）  
h0：隐藏层状态初始值(num_layers,batch_size,hidden_size)    
c0：记忆单元初始值(num_layers,batch_size,hidden_size)     



### GRU

### CW-RNN

## 双向化及加深网络

### 双向 RNN

### 深度双向 RNN





NLTK：自然语言工具包  

	from nltk.tokenize import sent_tokenize
	sentences = sent_tokenize(text)  # split text into sentences
	print(sentences)
	from nltk.tokenize import word_tokenize
	words = word_tokenize(text) # split text into words using NLTK
	print(words)

	