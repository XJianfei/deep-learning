{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow实现VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入需要使用的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convolution op wrapper, use RELU activation after convolution\n",
    "    Args:\n",
    "        layer_name: e.g. conv1, pool1...\n",
    "        x: input tensor, [batch_size, height, width, channels]\n",
    "        out_channels: number of output channels (or comvolutional kernels)\n",
    "        kernel_size: the size of convolutional kernel, VGG paper used: [3,3]\n",
    "        stride: A list of ints. 1-D of length 4. VGG paper used: [1, 1, 1, 1]\n",
    "        is_pretrain: if load pretrained parameters, freeze all conv layers. \n",
    "        Depending on different situations, you can just set part of conv layers to be freezed.\n",
    "        the parameters of freezed layers will not change when training.\n",
    "    Returns:\n",
    "        4D tensor\n",
    "'''\n",
    "def conv_layer(layer_name, x, out_channels, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=True):\n",
    "    in_channels = x.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights',\n",
    "                            trainable=is_pretrain,\n",
    "                            shape=[kernel_size[0], kernel_size[1], in_channels, out_channels],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer()) # default is uniform distribution initialization\n",
    "        b = tf.get_variable(name='biases',\n",
    "                            trainable=is_pretrain,\n",
    "                            shape=[out_channels],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        x = tf.nn.conv2d(x, w, stride, padding='SAME', name='conv')\n",
    "        x = tf.nn.bias_add(x, b, name='bias_add')\n",
    "\n",
    "        \n",
    "        x = tf.nn.relu(x, name='relu')\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pooling op\n",
    "    Args:\n",
    "        x: input tensor\n",
    "        kernel: pooling kernel, VGG paper used [1,2,2,1], the size of kernel is 2X2\n",
    "        stride: stride size, VGG paper used [1,2,2,1]\n",
    "        padding:\n",
    "        is_max_pool: boolen\n",
    "                    if True: use max pooling\n",
    "                    else: use avg pooling\n",
    "'''\n",
    "def pool(layer_name, x, kernel=[1,2,2,1], stride=[1,2,2,1], is_max_pool=True):\n",
    "    if is_max_pool:\n",
    "        x = tf.nn.max_pool(x, kernel, strides=stride, padding='SAME', name=layer_name)\n",
    "    else:\n",
    "        x = tf.nn.avg_pool(x, kernel, strides=stride, padding='SAME', name=layer_name)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义全连接层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Wrapper for fully connected layers with RELU activation as default\n",
    "    Args:\n",
    "        layer_name: e.g. 'FC1', 'FC2'\n",
    "        x: input feature map\n",
    "        out_nodes: number of neurons for current FC layer\n",
    "'''\n",
    "def fc_layer(layer_name, x, out_nodes,keep_prob=0.8):\n",
    "    shape = x.get_shape()\n",
    "    # 处理没有预先做flatten的输入\n",
    "    if len(shape) == 4:\n",
    "        size = shape[1].value * shape[2].value * shape[3].value\n",
    "    else:\n",
    "        size = shape[-1].value\n",
    "\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable('weights',\n",
    "                            shape=[size, out_nodes],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('biases',\n",
    "                            shape=[out_nodes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        \n",
    "        flat_x = tf.reshape(x, [-1, size]) # flatten into 1D\n",
    "        \n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义VGG16网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16_net(x, n_classes, is_pretrain=True):\n",
    "    with tf.name_scope('VGG16'):\n",
    "        x = conv_layer('conv1_1', x, 64, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        x = conv_layer('conv1_2', x, 64, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool1'):\n",
    "            x = pool('pool1', x, kernel=[1,2,2,1], stride=[1,2,2,1], is_max_pool=True)\n",
    "    \n",
    "        x = conv_layer('conv2_1', x, 128, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        x = conv_layer('conv2_2', x, 128, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool2'):\n",
    "            x = pool('pool2', x, kernel=[1,2,2,1], stride=[1,2,2,1], is_max_pool=True)\n",
    "    \n",
    "        x = conv_layer('conv3_1', x, 256, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        x = conv_layer('conv3_2', x, 256, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        x = conv_layer('conv3_3', x, 256, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool3'):\n",
    "            x = pool('pool3', x, kernel=[1,2,2,1], stride=[1,2,2,1], is_max_pool=True)\n",
    "    \n",
    "        x = conv_layer('conv4_1', x, 512, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        x = conv_layer('conv4_2', x, 512, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        x = conv_layer('conv4_3', x, 512, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool4'):\n",
    "            x = pool('pool4', x, kernel=[1,2,2,1], stride=[1,2,2,1], is_max_pool=True)\n",
    "    \n",
    "        x = conv_layer('conv5_1', x, 512, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        x = conv_layer('conv5_2', x, 512, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        x = conv_layer('conv5_3', x, 512, kernel_size=[3,3], stride=[1,1,1,1], is_pretrain=is_pretrain)\n",
    "        with tf.name_scope('pool5'):\n",
    "            x = pool('pool5', x, kernel=[1,2,2,1], stride=[1,2,2,1], is_max_pool=True)\n",
    "    \n",
    "        x = fc_layer('fc6', x, out_nodes=4096)\n",
    "        assert x.get_shape().as_list()[1:] == [4096]\n",
    "\n",
    "        x = fc_layer('fc7', x, out_nodes=4096)\n",
    "    \n",
    "        fc8 = fc_layer('fc8', x, out_nodes=n_classes)\n",
    "        # softmax = tf.nn.softmax(fc8)\n",
    "    \n",
    "    \n",
    "    return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义损失函数\n",
    "采用交叉熵计算损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Compute loss\n",
    "    Args:\n",
    "        logits: logits tensor, [batch_size, n_classes]\n",
    "        labels: one-hot labels\n",
    "'''\n",
    "def loss(logits, labels):\n",
    "    \n",
    "    with tf.name_scope('loss') as scope:\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels,name='cross-entropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope+'/loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Evaluate the quality of the logits at predicting the label.\n",
    "      Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor,\n",
    "'''\n",
    "def accuracy(logits, labels):\n",
    "    with tf.name_scope('accuracy') as scope:\n",
    "        correct = tf.equal(tf.arg_max(logits, 1), tf.arg_max(labels, 1))\n",
    "        correct = tf.cast(correct, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct)*100.0\n",
    "        tf.summary.scalar(scope+'/accuracy', accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(loss, learning_rate, global_step):\n",
    "    '''optimization, use Gradient Descent as default\n",
    "    '''\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义加载模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_skip(data_path, session, skip_layer):\n",
    "    data_dict = np.load(data_path, encoding='latin1').item()\n",
    "    for key in data_dict:\n",
    "        if key not in skip_layer:\n",
    "            with tf.variable_scope(key, reuse=True):\n",
    "                for subkey, data in zip(('weights', 'biases'), data_dict[key]):\n",
    "                    session.run(tf.get_variable(subkey).assign(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练图片读取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar10(data_dir, is_train, batch_size, shuffle):\n",
    "    \"\"\"Read CIFAR10\n",
    "    \n",
    "    Args:\n",
    "        data_dir: the directory of CIFAR10\n",
    "        is_train: boolen\n",
    "        batch_size:\n",
    "        shuffle:       \n",
    "    Returns:\n",
    "        label: 1D tensor, tf.int32\n",
    "        image: 4D tensor, [batch_size, height, width, 3], tf.float32\n",
    "    \n",
    "    \"\"\"\n",
    "    img_width = 32\n",
    "    img_height = 32\n",
    "    img_depth = 3\n",
    "    label_bytes = 1\n",
    "    image_bytes = img_width*img_height*img_depth\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        \n",
    "        if is_train:\n",
    "            filenames = [os.path.join(data_dir, 'data_batch_%d.bin' %ii)\n",
    "                                        for ii in np.arange(1, 6)]\n",
    "        else:\n",
    "            filenames = [os.path.join(data_dir, 'test_batch.bin')]\n",
    "          \n",
    "        filename_queue = tf.train.string_input_producer(filenames)\n",
    "    \n",
    "        reader = tf.FixedLengthRecordReader(label_bytes + image_bytes)\n",
    "    \n",
    "        key, value = reader.read(filename_queue)\n",
    "           \n",
    "        record_bytes = tf.decode_raw(value, tf.uint8)\n",
    "        \n",
    "        label = tf.slice(record_bytes, [0], [label_bytes])   \n",
    "        label = tf.cast(label, tf.int32)\n",
    "        \n",
    "        image_raw = tf.slice(record_bytes, [label_bytes], [image_bytes])     \n",
    "        image_raw = tf.reshape(image_raw, [img_depth, img_height, img_width])     \n",
    "        image = tf.transpose(image_raw, (1,2,0)) # convert from D/H/W to H/W/D       \n",
    "        image = tf.cast(image, tf.float32)\n",
    "\n",
    "     \n",
    "#        # data argumentation\n",
    "\n",
    "#        image = tf.random_crop(image, [24, 24, 3])# randomly crop the image size to 24 x 24\n",
    "#        image = tf.image.random_flip_left_right(image)\n",
    "#        image = tf.image.random_brightness(image, max_delta=63)\n",
    "#        image = tf.image.random_contrast(image,lower=0.2,upper=1.8)\n",
    "\n",
    "\n",
    "        \n",
    "        image = tf.image.per_image_standardization(image) #substract off the mean and divide by the variance \n",
    "\n",
    "\n",
    "        if shuffle:\n",
    "            images, label_batch = tf.train.shuffle_batch(\n",
    "                                    [image, label], \n",
    "                                    batch_size = batch_size,\n",
    "                                    num_threads= 64,\n",
    "                                    capacity = 20000,\n",
    "                                    min_after_dequeue = 3000)\n",
    "        else:\n",
    "            images, label_batch = tf.train.batch(\n",
    "                                    [image, label],\n",
    "                                    batch_size = batch_size,\n",
    "                                    num_threads = 64,\n",
    "                                    capacity= 2000)\n",
    "        ## ONE-HOT      \n",
    "        n_classes = 10\n",
    "        label_batch = tf.one_hot(label_batch, depth= n_classes)\n",
    "        label_batch = tf.cast(label_batch, dtype=tf.int32)\n",
    "        label_batch = tf.reshape(label_batch, [batch_size, n_classes])\n",
    "        \n",
    "        return images, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_W = 32\n",
    "IMG_H = 32\n",
    "N_CLASSES = 10\n",
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.01\n",
    "MAX_STEP = 10   # it took me about one hour to complete the training.\n",
    "IS_PRETRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224  # 输入图像尺寸\n",
    "images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, 3], dtype=tf.float32, stddev=1e-1))\n",
    "vgg16_net(images,keep_prob)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pre_trained_weights = './/vgg16_pretrain//vgg16.npy'\n",
    "    data_dir = './/data//cifar-10-batches-bin//'\n",
    "    train_log_dir = './/logs//train//'\n",
    "    val_log_dir = './/logs//val//'\n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        tra_image_batch, tra_label_batch = read_cifar10(data_dir=data_dir,\n",
    "                                                 is_train=True,\n",
    "                                                 batch_size= BATCH_SIZE,\n",
    "                                                 shuffle=True)\n",
    "        val_image_batch, val_label_batch = read_cifar10(data_dir=data_dir,\n",
    "                                                 is_train=False,\n",
    "                                                 batch_size= BATCH_SIZE,\n",
    "                                                 shuffle=False)\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_W, IMG_H, 3])\n",
    "    y_ = tf.placeholder(tf.int16, shape=[BATCH_SIZE, N_CLASSES])\n",
    "    \n",
    "      \n",
    "    logits = vgg16_net(x, N_CLASSES, IS_PRETRAIN)\n",
    "    loss_1 = loss(logits, y_)\n",
    "    accuracy = accuracy(logits, y_)\n",
    "    \n",
    "    my_global_step = tf.Variable(0, name='global_step', trainable=False) \n",
    "    train_op = optimize(loss_1, learning_rate, my_global_step)\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        print(x.shape())\n",
    "        print(y_.shape())\n",
    "        \n",
    "        if(IS_PRETRAIN):\n",
    "            load_with_skip(pre_trained_weights, sess, ['fc6','fc7','fc8']) \n",
    "        \n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        tra_summary_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "        val_summary_writer = tf.summary.FileWriter(val_log_dir, sess.graph)\n",
    "        \n",
    "        try:\n",
    "            for step in np.arange(MAX_STEP):\n",
    "                if coord.should_stop():\n",
    "                        break\n",
    "                    \n",
    "                tra_images,tra_labels = sess.run([tra_image_batch, tra_label_batch])\n",
    "                _, tra_loss, tra_acc = sess.run([train_op, loss, accuracy],\n",
    "                                                feed_dict={x:tra_images, y_:tra_labels})            \n",
    "                if step % 50 == 0 or (step + 1) == MAX_STEP:                 \n",
    "                    print ('Step: %d, loss: %.4f, accuracy: %.4f%%' % (step, tra_loss, tra_acc))\n",
    "                    summary_str = sess.run(summary_op)\n",
    "                    tra_summary_writer.add_summary(summary_str, step)\n",
    "                    \n",
    "                if step % 200 == 0 or (step + 1) == MAX_STEP:\n",
    "                    val_images, val_labels = sess.run([val_image_batch, val_label_batch])\n",
    "                    val_loss, val_acc = sess.run([loss, accuracy],\n",
    "                                                 feed_dict={x:val_images,y_:val_labels})\n",
    "                    print('**  Step %d, val loss = %.2f, val accuracy = %.2f%%  **' %(step, val_loss, val_acc))\n",
    "    \n",
    "                    summary_str = sess.run(summary_op)\n",
    "                    val_summary_writer.add_summary(summary_str, step)\n",
    "                        \n",
    "                if step % 2000 == 0 or (step + 1) == MAX_STEP:\n",
    "                    checkpoint_path = os.path.join(train_log_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path, global_step=step)\n",
    "                    \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "        \n",
    "        coord.join(threads)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_tensorflow_run(session, target, feed, info_string):\n",
    "    num_steps_burn_in = 10  # 预热轮数\n",
    "    total_duration = 0.0  # 总时间\n",
    "    total_duration_squared = 0.0  # 总时间的平方和用以计算方差\n",
    "    for i in range(num_batches + num_steps_burn_in):\n",
    "        start_time = time.time()\n",
    "        _ = session.run(target,feed_dict=feed)\n",
    "        duration = time.time() - start_time\n",
    "        if i >= num_steps_burn_in:  # 只考虑预热轮数之后的时间\n",
    "            if not i % 10:\n",
    "                print('%s:step %d,duration = %.3f' % (datetime.now(), i - num_steps_burn_in, duration))\n",
    "                total_duration += duration\n",
    "                total_duration_squared += duration * duration\n",
    "    mn = total_duration / num_batches  # 平均每个batch的时间\n",
    "    vr = total_duration_squared / num_batches - mn * mn  # 方差\n",
    "    sd = math.sqrt(vr)  # 标准差\n",
    "    print('%s: %s across %d steps, %.3f +/- %.3f sec/batch' % (datetime.now(), info_string, num_batches, mn, sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark():\n",
    "    with tf.Graph().as_default():\n",
    "        '''定义图片尺寸224，利用tf.random_normal函数生成标准差为0.1的正态分布的随机数来构建224x224的随机图片'''\n",
    "        image_size = 224  # 输入图像尺寸\n",
    "        images = tf.Variable(tf.random_normal([batch_size, image_size, image_size, 3], dtype=tf.float32, stddev=1e-1))\n",
    "        #构建keep_prob的placeholder\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        prediction,softmax,fc8,p = vgg16_net(images,keep_prob)\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "        #设置keep_prob为1.0，运用time_tensorflow_run来评测forward运算随机\n",
    "        time_tensorflow_run(sess, prediction,{keep_prob:1.0}, \"Forward\")\n",
    "        # 用以模拟训练的过程\n",
    "        objective = tf.nn.l2_loss(fc8)  # 给一个loss\n",
    "        grad = tf.gradients(objective, p)  # 相对于loss的 所有模型参数的梯度\n",
    "        #评测backward运算时间\n",
    "        time_tensorflow_run(sess, grad, {keep_prob:0.5},\"Forward-backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_batches = 100\n",
    "run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    " \n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    " \n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    " \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/roguesir/article/details/77051250\n",
    "https://blog.csdn.net/zhangwei15hh/article/details/78417789\n",
    "https://blog.csdn.net/v1_vivian/article/details/77898652"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
